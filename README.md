# llm-comparison-web-app
Web app to compare responses of multiple LLMs via OpenRouter
# ðŸ§  LLM Comparison Web App

Compare responses from multiple Large Language Models (LLMs) like GPT-4, Claude, LLaMA, and Gemini â€” side-by-side, in real time â€” using the OpenRouter API.

---

## ðŸš€ Features

- âœ… Enter any prompt
- âœ… Choose 2 LLMs from a dropdown
- âœ… See real model outputs, side-by-side
- âœ… Optional: Google OAuth login
- âœ… Powered by OpenRouter API

---

## ðŸ›  Tech Stack

| Layer       | Technology            |
|-------------|------------------------|
| Frontend    | React (Vite)           |
| Backend     | FastAPI (Python)       |
| Auth        | Google OAuth (Authlib) |
| LLM Access  | OpenRouter API         |
| HTTP Client | Axios (frontend), httpx (backend) |
| Versioning  | Git + GitHub           |

---

## ðŸ“¦ Folder Structure

llm-comparison-web-app/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ routers/
â”‚ â”œâ”€â”€ .env.example
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ client/
â”‚ â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ components/
â”‚ â””â”€â”€ App.jsx
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md


---

## ðŸ§ª How It Works

1. User enters a prompt
2. Selects two LLMs from dropdown
3. Frontend sends two POST requests to backend
4. Backend hits OpenRouter API for both models
5. Responses are returned and displayed side-by-side

---

## ðŸ§° How to Run Locally

### âœ… Backend (FastAPI)

cd backend
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
uvicorn main:app --reload

### âœ… Frontend (React + Vite)

cd frontend/client
npm install
npm run dev

